{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMarL+85EOGdt044gNaSjxY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarakantaacharya/Stock_Movement_Analysis/blob/main/Model_Building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "EtNq1e6806iW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Instructions :\n",
        "1. You should run the Data Scrapping, Data_Preprocessing_Cleaning and Feature Engineering files, Dataset for Model Building files before Model Building for required data...\n",
        "2. Load the required datasets if you dont want to rebuild the datatset...\n",
        "3. Installl the required libraries from requirements.txt"
      ],
      "metadata": {
        "id": "s01wK8Tk4fW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the TensorFlow library, which is essential for building and training deep learning models.\n",
        "!pip install tensorflow\n",
        "# Install the Scikeras library, which provides an interface to use Keras models within scikit-learn pipelines for easier machine learning integration.\n",
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ee5be4-0a49-4b3c-88db-04158d53ff79",
        "id": "EpO5Dsvo1d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Importing Libraries"
      ],
      "metadata": {
        "id": "vdx7J_e81QRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the pandas library for data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Importing various classification algorithms from scikit-learn\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier  # Ensemble learning models\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic regression model\n",
        "from sklearn.svm import SVC  # Support Vector Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors Classifier\n",
        "\n",
        "# Importing evaluation metrics to assess model performance\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Importing train_test_split for splitting the dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Importing StandardScaler to standardize features by removing the mean and scaling to unit variance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Importing TensorFlow's Keras library for building deep learning models\n",
        "from tensorflow.keras.models import Sequential  # Sequential API for building models layer by layer\n",
        "\n",
        "# Importing layers from Keras for building neural network architectures\n",
        "from keras.layers import Dense, Dropout, Input  # Dense: fully connected layers, Dropout: prevents overfitting\n",
        "\n",
        "# Importing Adam optimizer for training deep learning models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Importing KerasClassifier wrapper to use Keras models in scikit-learn pipelines\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Importing StratifiedKFold and cross_val_score for cross-validation\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score  # StratifiedKFold ensures balanced class distribution in splits\n",
        "\n",
        "# Importing visualization libraries\n",
        "from matplotlib import pyplot as plt  # For creating visualizations\n",
        "import seaborn as sns  # Advanced visualization library with aesthetic options\n",
        "\n",
        "# Importing train_test_split again (redundant here since already imported earlier)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Importing make_pipeline to create machine learning pipelines\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Importing StandardScaler again (redundant here since already imported earlier)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Creating an instance of StandardScaler for feature scaling\n",
        "scaler = StandardScaler()  # Standardizes features to have mean 0 and standard deviation 1"
      ],
      "metadata": {
        "id": "jHv1_y9m1Niq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features and target variable\n",
        "X_features = ['score', 'num_comments', 'upvote_ratio', 'title_sentiment_score',\n",
        "       'content_sentiment_score', 'Close_AAPL', 'Price_Change', 'Prev_Price_Change']\n",
        "\n",
        "X = model_df[X_features]  # Adjust features based on your data\n",
        "y = model_df['stock_direction']\n",
        "\n",
        "# Train-test split (60-40 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHnWY7f2x9_a",
        "outputId": "4dbea697-6d8d-4921-f2b4-c42bb9b4fdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (4502, 8)\n",
            "X_test shape: (3002, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df_1 = pd.DataFrame()   #We create an empty dataframe to store the metric results"
      ],
      "metadata": {
        "id": "yTztaEoWZEXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Explanation of the models:\n",
        "1. Random Forest: A robust ensemble method that combines multiple decision trees to improve classification accuracy and reduce overfitting. class_weight='balanced' automatically adjusts class weights inversely proportional to their frequencies in the data.\n",
        "\n",
        "2. Gradient Boosting: Builds models sequentially, optimizing for residual errors. It's often used for competitive performance in structured data.\n",
        "\n",
        "3. AdaBoost: Combines weak classifiers iteratively to focus on misclassified instances. The SAMME algorithm supports multi-class outputs.\n",
        "\n",
        "4. Logistic Regression: A linear model used for binary/multi-class classification. Here, it's combined with StandardScaler for preprocessing, and saga is chosen for its efficiency on large datasets.\n",
        "\n",
        "5. Support Vector Machine (SVC): Useful for high-dimensional spaces and non-linear decision boundaries. The class_weight='balanced' adjusts for class imbalance.\n",
        "\n",
        "6. K-Nearest Neighbors: Simple and intuitive, relying on the proximity of data points. The choice of n_neighbors=5 is a common default, but it can be tuned.\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "#####Additional Notes:\n",
        "1. Class weights: Models like Random Forest, SVC, and Logistic Regression are set with class_weight='balanced' to handle datasets with imbalanced target distributions effectively.\n",
        "2. Random State: Ensures reproducibility for models that involve randomness.\n",
        "Pipelines: Used for Logistic Regression to combine preprocessing (scaling) and modeling into a single step."
      ],
      "metadata": {
        "id": "2LJNvo71uCgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a dictionary of machine learning models with specific hyperparameters\n",
        "models = {\n",
        "    # Random Forest: Ensemble model using multiple decision trees, with 100 trees and balanced class weights\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        n_estimators=100,  # Number of decision trees\n",
        "        class_weight='balanced',  # Adjust weights for imbalanced classes\n",
        "        random_state=42  # Ensures reproducibility\n",
        "    ),\n",
        "\n",
        "    # Gradient Boosting: Ensemble model where trees are built sequentially to minimize errors\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
        "        n_estimators=100,  # Number of boosting stages\n",
        "        random_state=42  # Ensures reproducibility\n",
        "    ),\n",
        "\n",
        "    # AdaBoost: Boosting algorithm with SAMME (for multi-class classification)\n",
        "    \"AdaBoost\": AdaBoostClassifier(\n",
        "        algorithm='SAMME'  # Algorithm type, SAMME is suitable for multi-class problems\n",
        "    ),\n",
        "\n",
        "    # Logistic Regression: Linear model wrapped in a pipeline with scaling and custom parameters\n",
        "    'Logistic Regression': make_pipeline(\n",
        "        StandardScaler(),  # Standardizes features\n",
        "        LogisticRegression(\n",
        "            max_iter=3000,  # Maximum number of iterations for optimization\n",
        "            solver='saga',  # Solver suitable for large datasets and supports L1/L2 regularization\n",
        "            class_weight='balanced',  # Adjust weights for imbalanced classes\n",
        "            random_state=42  # Ensures reproducibility\n",
        "        )\n",
        "    ),\n",
        "\n",
        "    # Support Vector Machine: Non-linear classifier with kernel tricks\n",
        "    \"Support Vector Machine\": SVC(\n",
        "        class_weight='balanced',  # Adjust weights for imbalanced classes\n",
        "        random_state=42  # Ensures reproducibility\n",
        "    ),\n",
        "\n",
        "    # K-Nearest Neighbors: Distance-based algorithm, finding the 5 nearest neighbors\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(\n",
        "        n_neighbors=5  # Number of neighbors to consider\n",
        "    )\n",
        "}"
      ],
      "metadata": {
        "id": "9VaIYKpWxstq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Explanation of the DNN:\n",
        "1. Input Layer: The Input layer specifies the shape of input data, which corresponds to the number of features in the dataset (X.shape[1]).\n",
        "\n",
        "2. Hidden Layers:\n",
        "\n",
        "    2.1 First hidden layer: 64 neurons, ReLU activation for non-linearity, followed by a Dropout layer to mitigate overfitting.\n",
        "\n",
        "    2.2 Second hidden layer: 32 neurons, ReLU activation, with another Dropout layer.\n",
        "\n",
        "3. Output Layer:\n",
        "A single neuron with a sigmoid activation function to output probabilities, suitable for binary classification.\n",
        "\n",
        "4. Model Compilation:\n",
        "\n",
        "    4.1 Optimizer: Adam is chosen for its adaptive learning rate and efficiency.\n",
        "\n",
        "    4.2 Loss function: Binary cross-entropy is appropriate for binary classification tasks.\n",
        "\n",
        "    4.3 Metrics: Accuracy is used to evaluate the model during training.\n",
        "\n",
        "#####Integration with scikit-learn:\n",
        "The KerasClassifier wrapper enables the DNN to integrate seamlessly into scikit-learn pipelines, making it compatible with functions like cross-validation and hyperparameter tuning.\n",
        "\n",
        "#####Adding to the models dictionary:\n",
        "The DNN model is added under the key \"Deep Neural Network\" to be evaluated alongside other machine learning models."
      ],
      "metadata": {
        "id": "Yiy5G5jkuh68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to build the Deep Neural Network (DNN) model\n",
        "def build_dnn():\n",
        "    # Define the model structure using Keras Sequential API\n",
        "    model = Sequential([\n",
        "        # Input layer: Automatically adjusts to the number of features in the dataset\n",
        "        Input(shape=(X.shape[1],)),  # Input layer with shape matching the number of features in X\n",
        "\n",
        "        # First hidden layer: 64 neurons with ReLU activation for non-linearity\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),  # Dropout layer to reduce overfitting by randomly dropping 20% of neurons\n",
        "\n",
        "        # Second hidden layer: 32 neurons with ReLU activation\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),  # Dropout for further regularization\n",
        "\n",
        "        # Output layer: 1 neuron with sigmoid activation for binary classification\n",
        "        Dense(1, activation='sigmoid')  # Outputs probability of the positive class\n",
        "    ])\n",
        "\n",
        "    # Compile the model with the Adam optimizer and binary cross-entropy loss\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),  # Optimizer with a learning rate of 0.001\n",
        "        loss='binary_crossentropy',  # Loss function for binary classification\n",
        "        metrics=['accuracy']  # Evaluation metric to track during training\n",
        "    )\n",
        "    return model  # Return the constructed model\n",
        "\n",
        "# Wrap the DNN model with KerasClassifier for compatibility with scikit-learn workflows\n",
        "dnn_model = KerasClassifier(\n",
        "    model=build_dnn,  # The function that defines the DNN architecture\n",
        "    epochs=25,  # Number of training epochs\n",
        "    batch_size=32,  # Mini-batch size for gradient updates\n",
        "    verbose=0  # Suppress training output\n",
        ")\n",
        "\n",
        "# Add the DNN model to the dictionary of models for evaluation\n",
        "models['Deep Neural Network'] = dnn_model"
      ],
      "metadata": {
        "id": "JZ-t4oDvdBZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here after defining the respective model ....\n",
        "In next step we will train the defined model with refined model_df dataset"
      ],
      "metadata": {
        "id": "LTwkFGj22hYV"
      }
    }
  ]
}